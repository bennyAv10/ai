{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bennyav/learn-ai-today-01?scriptVersionId=181713204\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 01 - Getting started with PyTorch\n\nhttps://towardsdatascience.com/learn-ai-today-01-getting-started-with-pytorch-2e3ba25a518","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\nimport plotly.graph_objects as go\nfrom torch import optim\nimport torch.nn.functional as F\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T20:49:21.486882Z","iopub.execute_input":"2024-06-05T20:49:21.488071Z","iopub.status.idle":"2024-06-05T20:49:21.496821Z","shell.execute_reply.started":"2024-06-05T20:49:21.488022Z","shell.execute_reply":"2024-06-05T20:49:21.495721Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n# Original code uses Module from fastai\nclass LinearRegression(nn.Module):\n    def __init__(self, number_of_inputs, number_of_outputs):\n        super().__init__()\n        self.linear = nn.Linear(number_of_inputs, number_of_outputs)\n\n    def forward(self, x):\n        return self.linear(x)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef fit(inputs, targets, model, criterion, optimizer, num_epochs):\n    \"\"\"\n    Train the `model` with the train dataset `(inputs, targets)`\n    'criterion' in the function used to calculate the loss\n    `optimizer` changes the model weights based on the loss\n    \"\"\"\n    loss_history = []\n    out_history = []\n    \n    first_iteration = True\n    \n    for epoch in range(num_epochs):\n        # forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        \n        # backward pass\n        optimizer.zero_grad()\n        # what exactly is done on this step\n        loss.backward()\n        # How the optimizer is aware of the loss\n        optimizer.step()\n        \n        loss_history.append(loss.item())\n        #print(\"out:\", outputs.detach().numpy())\n        if first_iteration:\n            out_history = outputs.detach().numpy()\n            first_iteration=False\n        else:\n            out_history = np.concatenate((out_history, outputs.detach().numpy()), axis=1)\n        #print('Epoch {}/{}, Loss {:.6f}'.format(epoch, num_epochs, loss.item()))\n    return (out_history, loss_history)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression(1,1)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare the training data\n\nx_train = torch.linspace(0,1,10000)\ntrue_y = 2 * x_train + 1  # True relationship\n# Observed y is 2X+1 + some noise\ny_train = 2*x_train + 1 + torch.randn(x_train.size())*x_train*0.1\nx_train_ready = x_train.unsqueeze(-1)\ny_train_ready =  y_train.unsqueeze(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization functions","metadata":{}},{"cell_type":"code","source":"# Just a static plot of noisy data (Observations) grey scatter, real function - blue line, and predicted function - red line\ndef matplot_prediction_ground_noise(x, prediction, ground_truth, observations):\n    plt.plot(x, ground_truth, color='blue')\n    plt.plot(x, prediction, color='red')\n    plt.scatter(x, observations, color='gray')\n    plt.grid()\n    plt.legend(['Prediction', 'Observation'])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotly_prediction_ground_noise(x, *, prediction=None, ground_truth=None, observations=None):\n    \"\"\"\n    Plots observation, ground truth, and predictions over a graph using plotly\n    \"\"\"\n    # Create the figure\n    fig = go.Figure()\n    \n    Legend = []\n    if observations is not None:\n        # Add noisy data points\n        fig.add_trace(go.Scatter(x=x, y=observations, mode='markers', name='Noisy data (model inputs)', marker=dict(color='gray')))\n\n    if ground_truth is not None:\n        # Add true data line\n        fig.add_trace(go.Scatter(x=x, y=ground_truth, mode='lines', name='True data', line=dict(color='black')))\n        \n    if prediction is not None:\n        # Add predicted line\n        fig.add_trace(go.Scatter(x=x, y=prediction, mode='lines', name='Prediction', line=dict(color='rgba(255, 0, 0, 0.5)')))\n\n    # Update layout\n    fig.update_layout(\n        title='True Data vs Noisy Data',\n        xaxis_title='X',\n        yaxis_title='Y = 2x + 1',\n        legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,200,255,0.5)')\n    )\n\n    # Show the plot\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotly_loss_over_time(loss_history):\n    # what happens to the loss over epochs\n    loss_fig = go.Figure()\n    loss_fig.add_trace(go.Scatter(x=[i for i in range(len(loss))],y=loss_history, mode='lines', name='Loss Per Epoch', line=dict(color='red') ))\n    loss_fig.update_layout(\n        title='Loss per Epoch',\n        xaxis_title='Epoch',\n        yaxis_title='Loss',\n        legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,200,255,0.5)'),\n        showlegend=True\n    )\n    loss_fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_animated_prediction(x, *, ground_truth, observations, prediction_per_epoch):\n    \"\"\"\n    Plot animation of prediction per epoch together with the ground truth and observations (Noise)\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(6,3), dpi=120)\n    #ax.set_xlim((-0.1, 1.1))\n    #ax.set_ylim((-5, 20))\n    ax.plot(x, ground_truth, lw=2, color='black', label='True model')\n    ax.scatter(x, observations, s=10, alpha=0.5, color='gray', label='Observed data') \n    ax.set_ylabel(r'$y = 3x^2 + 2x + 1 + noise$') # Notice you can use latex in the label string\n    line, = ax.plot([], [], lw=2, label='Predicted model')\n    ax.legend()\n\n    # animation function. This is called sequentially\n    def animate(i):\n        line.set_data(x, prediction_per_epoch[...,i])\n        return (line,)\n\n    # call the animator. blit=True means only re-draw the parts that have changed.\n    anim = animation.FuncAnimation(fig, animate, frames=out.shape[1], interval=30, blit=True)\n    return anim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the data for visualization\nx = x_train.numpy()\ntrue_y = 2 * x + 1  # True relationship\n\nnoisy_y = y_train.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Alternatively use the above plotly function\nplotly_prediction_ground_noise(x, ground_truth=true_y, observations=noisy_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's get our hands dirty","metadata":{}},{"cell_type":"code","source":"# Train\n\n# what is requires_grad?\n(out, loss) = fit(inputs=x_train_ready.requires_grad_(True), targets=y_train_ready, model=model, criterion=criterion, optimizer=optimizer, num_epochs=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotly_loss_over_time(loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot the most uptodate over the trainning data\n# interestingly, before calling this, the model didn't generate it correctly\n#model.eval()\n#outputs = model(x_train_ready)\n\n# Create the figure\nanimated_fig = go.Figure()\n\n# Add noisy data points\nnoisy_trace = go.Scatter(x=x, y=noisy_y, mode='markers', name='Noisy data (model inputs)', marker=dict(color='gray'))\nanimated_fig.add_trace(noisy_trace)\n\n# Add true data line\ntrue_data_trace = go.Scatter(x=x, y=true_y, mode='lines', name='True data', line=dict(color='black'))\nanimated_fig.add_trace(true_data_trace)\n\n# Add the prediction line\nprediction_trace = go.Scatter(x=[None], y=[None], mode='lines', name='Epoch Prediction', line=dict(color='blue'))\nanimated_fig.add_trace(prediction_trace)\n\nframes = []\nfor i in range(len(out[0])):\n    frames.append(go.Frame(data=[\n        noisy_trace,\n        go.Scatter(x=x_train.numpy(), y=out[..., i], mode='lines', name='Epoch Prediction', line=dict(color='blue'))],\n                 name=f'Epoch {i}'))\n\nanimated_fig.update(frames=frames)\nanimated_fig.update_layout(\n    title='True Data vs Noisy Data',\n    xaxis_title='X',\n    yaxis_title='Y = 2x + 1',\n    legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.5)'),\n    updatemenus=[{\n        'type': 'buttons',\n        'showactive': False,\n        'buttons': [{\n            'label': 'Play',\n            'method': 'animate',\n            'args': [None, {'frame': {'duration': 50, 'redraw': False}, 'fromcurrent': True}]\n            #'args': [None]\n        }]\n    }])\nanimated_fig.show()\n#animated_fig.write_html(file='animated_plot.html', auto_open=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture \nanim = plot_animated_prediction(x, ground_truth=true_y, observations=noisy_y, prediction_per_epoch=out)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(anim.to_html5_video())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's do ploynomial regression","metadata":{}},{"cell_type":"code","source":"# prepare the data\n\npoly_x_true = torch.linspace(-2, 2, 1000)\npoly_y_true = 3*poly_x_true**2 + 2*poly_x_true + 1\npoly_y_train = poly_y_true + torch.randn(poly_y_true.size())\npoly_y_train.unsqueeze_(-1)\n\n# The way to predict a polynom with linear regression is by having two free variables: x, and X^2\npoly_x_train = torch.cat((poly_x_true.unsqueeze(-1)**2, poly_x_true.unsqueeze(-1)), dim=1)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotly_prediction_ground_noise(poly_x_true, ground_truth=poly_y_true, observations=poly_y_train.squeeze(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\npoly_model = LinearRegression(2, 1)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(poly_model.parameters(), lr=0.1)\n\n%time (out, loss) = fit(inputs=poly_x_train, targets=poly_y_train, model=poly_model, criterion=criterion, optimizer=optimizer, num_epochs=250)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotly_loss_over_time(loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poly_model.eval()\nye = poly_model(poly_x_train)\nye=ye.detach().squeeze(1).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotly_prediction_ground_noise(poly_x_true, ground_truth=poly_y_true, observations=poly_y_train.squeeze(1), prediction=ye)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture \nanim = plot_animated_prediction(poly_x_true, ground_truth=poly_y_true, observations=poly_y_train, prediction_per_epoch=out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(anim.to_html5_video())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(poly_model.parameters()))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network","metadata":{}},{"cell_type":"code","source":"class GeneralFit(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size=100):\n        super().__init__()\n        self.linear_in = nn.Linear(input_size, hidden_size)\n        self.hidden = nn.Linear(hidden_size, hidden_size)\n        self.linear_out = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        x = torch.relu(self.linear_in(x))\n        x = torch.relu(self.hidden(x))\n        y = self.linear_out(x)\n        \n        return y\n        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cubic Polynom","metadata":{}},{"cell_type":"code","source":"# prepare the data\n\ncubic_x = torch.linspace(-2, 2, 1000).unsqueeze(-1)\ncubic_y_true = 6*cubic_x**3 + 3*cubic_x**2 + 2*cubic_x + 1\ncubic_y_train = cubic_y_true + torch.randn(cubic_y_true.size())\n\nprint(cubic_x.size(), cubic_y_train.size())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cubic_x[..., 0].size())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets plot true and noisy Y\nplotly_prediction_ground_noise(cubic_x.squeeze(1), ground_truth=cubic_y_true.squeeze(1), observations=cubic_y_train.squeeze(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ncubic_model_nn = GeneralFit(1,1)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(cubic_model_nn.parameters(), lr=0.05)\n\n%time (cubic_nn_out, cubic_nn_loss) = fit(inputs=cubic_x, targets=cubic_y_train, model=cubic_model_nn, criterion=criterion, optimizer=optimizer, num_epochs=500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotly_loss_over_time(cubic_nn_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cubic_model_nn.eval()\n%time cubic_nn_y_prediction = cubic_model_nn(cubic_x).detach()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot prediction alongside observations and ground truth\nplotly_prediction_ground_noise(cubic_x.squeeze(1), \n                               ground_truth=cubic_y_true.squeeze(1), \n                               observations=cubic_y_train.squeeze(1),\n                               prediction=cubic_nn_y_prediction.squeeze(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture \nanim = plot_animated_prediction(cubic_x.squeeze(1), \n                                ground_truth=cubic_y_true.squeeze(1), \n                                observations=cubic_y_train.squeeze(1), \n                                prediction_per_epoch=cubic_nn_out)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(anim.to_html5_video())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cubic Polynom with a linear regression model\nWe just used the `GeneralFit` model to train for a cubic polynom.\nCan we use the linear regression model we used earlier for the same purpose using the same method we used to train it for quadratic polynom","metadata":{}},{"cell_type":"code","source":"# The way to predict a polynom with linear regression is by having three free variables: x, and X^2\ncubic_x_linear = torch.cat((cubic_x**3, cubic_x**2, cubic_x), dim=1)\n\n# Train the model\ncubic_model_linear = LinearRegression(3,1)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(cubic_model_linear.parameters(), lr=0.01)\n\n%time (cubic_linear_out, cubic_linear_loss) = fit(inputs=cubic_x_linear, targets=cubic_y_train, model=cubic_model_linear, criterion=criterion, optimizer=optimizer, num_epochs=250)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotly_loss_over_time(cubic_linear_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}